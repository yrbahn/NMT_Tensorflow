{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EASY_MODE = True\n",
    "\n",
    "MODE = \"he-to-en\"\n",
    "END = ';'  \n",
    "MAX_OUTPUT_LENGTH = 50 if not EASY_MODE else 20\n",
    "REPORT_FREQ       = 100    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1.  전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\tאנרכיזם\r\n",
      "Autism\tאוטיזם קלאסי\r\n",
      "Albedo\tאלבדו\r\n",
      "A\tA\r\n",
      "Alabama\tאלבמה\r\n",
      "Achilles\tאכילס\r\n",
      "Abraham Lincoln\tאברהם לינקולן\r\n",
      "Aristotle\tאריסטו\r\n",
      "An American in Paris\tאמריקאי בפריז\r\n",
      "Academy Awards\tפרס אוסקר\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 10 data/main_dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size =  130699\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "word_to_translation = defaultdict(list)\n",
    "\n",
    "with open(\"data/main_dataset.txt\") as fin:\n",
    "    for line in fin:\n",
    "        en, he = line[:-1].lower().replace(END,' ').split('\\t')\n",
    "        word, trans = (he,en) if MODE=='he-to-en' else (en,he)\n",
    "        if EASY_MODE:\n",
    "            if max(len(word), len(trans)) > 20:\n",
    "                continue\n",
    "        word_to_translation[word+END].append(trans+END)\n",
    "        \n",
    "print(\"size = \", len(word_to_translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': 0, 'あ': 1, 'ぼ': 2, 'ê': 3, '一': 4, 'x': 5, 'א': 6, '×': 7, '°': 8, 'т': 9, '4': 10, 'ז': 11, 'a': 12, 'ґ': 13, 'ר': 14, 'ט': 15, 'チ': 16, 'ғ': 17, 'っ': 18, 'ו': 19, 'k': 20, '子': 21, 'ж': 22, 'ѻ': 23, '=': 24, '守': 25, 'ֿ': 26, 'б': 27, '−': 28, 'y': 29, 'ן': 30, 'н': 31, 'm': 32, 'ĵ': 33, 'þ': 34, 'č': 35, 'u': 36, 'д': 37, 'ĉ': 38, 'ׁ': 39, 'マ': 40, 'ָ': 41, '·': 42, '•': 43, 'у': 44, 'ぺ': 45, 'ױ': 46, 'к': 47, 'ص': 48, 'ѯ': 49, 'و': 50, 'á': 51, ' ': 52, '7': 53, 'f': 54, '3': 55, 'я': 56, 'ш': 57, 'ю': 58, '桜': 59, 'ç': 60, 'ד': 61, 'פ': 62, '0': 63, 'ц': 64, '’': 65, 'ø': 66, '榎': 67, 'ӡ': 68, 'כ': 69, 'س': 70, 'ث': 71, '仙': 72, 'ץ': 73, 'ÿ': 74, '野': 75, 'ش': 76, 'o': 77, 'd': 78, 'š': 79, 'í': 80, 'g': 81, 'ֵ': 82, 'る': 83, '猫': 84, 'ק': 85, 'ъ': 86, '$': 87, 't': 88, 'ع': 89, 'ĝ': 90, 'w': 91, '2': 92, '(': 93, 'コ': 94, 'ر': 95, 'è': 96, 'с': 97, 'ּ': 98, '@': 99, 'з': 100, '#': 101, 'ב': 102, 'à': 103, '門': 104, 'گ': 105, 'ò': 106, '8': 107, 'ע': 108, 'ظ': 109, 'ら': 110, 'ь': 111, 'р': 112, 'n': 113, '′': 114, ')': 115, '&': 116, '.': 117, 'ء': 118, 'ף': 119, 'ف': 120, 'о': 121, 's': 122, 'ס': 123, 'ѓ': 124, 'ń': 125, '־': 126, 'ם': 127, 'ӎ': 128, 'r': 129, 'ג': 130, '\"': 131, 'ə': 132, 'ấ': 133, 'ы': 134, 'י': 135, 'v': 136, 'л': 137, '\\\\': 138, 'г': 139, 'ח': 140, 'в': 141, 'と': 142, 'ö': 143, 'и': 144, 'ک': 145, 'а': 146, 'ذ': 147, 'ộ': 148, 'ŭ': 149, '/': 150, 'צ': 151, 'п': 152, '熊': 153, '5': 154, '!': 155, 'е': 156, 'ִ': 157, 'ҙ': 158, 'ó': 159, \"'\": 160, 'џ': 161, 'l': 162, 'p': 163, 'щ': 164, 'ß': 165, 'ﭪ': 166, 'ل': 167, 'i': 168, 'ú': 169, ',': 170, 'e': 171, '+': 172, '∂': 173, '星': 174, 'タ': 175, 'q': 176, 'ü': 177, ';': 178, 'خ': 179, ':': 180, '6': 181, '²': 182, 'غ': 183, 'ﭺ': 184, '1': 185, 'μ': 186, 'ژ': 187, 'b': 188, 'é': 189, 'z': 190, 'ך': 191, 'ŋ': 192, 'j': 193, 'ق': 194, 'ф': 195, 'ﭖ': 196, 'מ': 197, 'ě': 198, 'х': 199, '9': 200, 'い': 201, 'ת': 202, 'ی': 203, 'ש': 204, '″': 205, 'h': 206, 'ا': 207, '≤': 208, '½': 209, 'ה': 210, 'ŝ': 211, 'ӟ': 212, 'נ': 213, 'ל': 214, 'ñ': 215, 'ض': 216, 'م': 217, 'ت': 218, '?': 219, '*': 220, '御': 221, '♯': 222, 'ַ': 223, 'ѳ': 224, '白': 225, '-': 226, '–': 227, '׳': 228, 'ө': 229, '¡': 230}\n"
     ]
    }
   ],
   "source": [
    "# 소스 언어에서 모든 유니크한 문자를 구하자(a.k.a 소스 사전)\n",
    "all_words = list(word_to_translation.keys())\n",
    "source_letters = list(set(''.join(all_words)))\n",
    "source_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "print(source_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ḳ', 'c', 'ʾ', 'ā', 'ο', 'ḍ', 'あ', 'ḏ', 'ê', 'ぼ', '一', 'ʻ', 'x', 'η', 'א', 'ρ', 'ğ', '×', 'ģ', 'զ', '°', 'ď', '4', 'т', 'ז', 'ŏ', 'a', 'ר', 'ט', 'チ', 'ė', 'っ', 'ä', 'ì', 'k', 'ו', '→', '=', 'ã', '子', 'û', '守', '−', 'y', 'ň', 'ź', 'ư', 'œ', 'ḥ', 'ן', 'm', 'վ', 'ĵ', 'ớ', 'þ', 'č', 'н', 'u', 'ồ', 'ĉ', 'ř', 'マ', '·', '≤', '•', 'ぺ', 'ş', 'у', 'ż', '♭', 'ķ', 'ա', 'к', 'ć', 'ص', 'ý', 'و', 'á', 'ơ', ' ', '7', 'τ', 'ɱ', 'f', '3', 'ï', 'я', 'ш', 'ţ', 'ю', '桜', 'ç', 'ד', 'פ', '0', 'ц', '’', 'ø', 'ă', 'å', 'σ', 'כ', '榎', 'س', '仙', 'ÿ', 'ղ', '野', 'o', 'd', '~', 'š', 'í', 'g', 'π', 'る', '猫', 'ק', '…', 'ù', 'ъ', '$', 't', 'ų', 'ū', 'ع', 'δ', 'ν', 'ễ', 'ʿ', 'ĝ', 'ž', 'w', '2', '(', 'コ', 'ر', 'è', 'ẓ', 'ů', '@', 'ב', '—', 'à', '門', 'ế', 'ņ', 'ò', '8', 'ע', 'ạ', 'ら', 'ь', 'р', 'n', 'ð', 'ľ', '′', 'æ', ')', '&', '.', 'ف', 'ף', 'о', 'ζ', 's', 'ļ', 'ı', 'ṯ', 'ס', 'ή', 'ń', 'ם', 'µ', 'ť', 'r', '\"', 'ấ', 'ə', '⋯', 'ג', 'ы', 'י', 'v', 'л', 'ö', 'ח', 'в', 'と', 'и', 'ک', 'а', 'ộ', 'ś', 'ō', 'ŭ', '/', 'ę', 'п', 'צ', '熊', '5', '‘', '!', 'ô', '̇', 'ħ', 'е', 'ó', '`', \"'\", 'ḫ', 'l', 'կ', 'ṇ', 'p', 'ṃ', 'õ', '%', 'ș', 'ē', 'ứ', 'ß', 'ل', 'i', 'υ', 'ú', ',', 'đ', '¡', 'ł', 'e', '+', '∂', '星', 'ր', 'タ', 'q', 'ü', 'ا', 'ỏ', ';', ':', '6', 'ả', '²', '1', 'μ', 'b', 'ő', 'ỹ', 'é', '^', 'z', 'ŋ', 'j', 'ق', 'ọ', 'ф', 'מ', 'ě', 'い', '9', 'ی', 'ת', 'ë', '″', 'ש', 'h', '陳', 'ε', 'â', 'ą', '½', 'ŝ', 'ה', 'ű', 'î', '老', 'ל', 'נ', 'ñ', 'م', 'ī', 'ت', '*', '?', '御', '白', '-', '–', 'ț']\n"
     ]
    }
   ],
   "source": [
    "#타켓 언어에서 모든 유니크한 문자를 구하자\n",
    "all_translations = [ts for all_ts in word_to_translation.values() for ts in all_ts]\n",
    "target_letters = list(set([l for ts in all_translations for l in ts] + [\" \"]))\n",
    "target_to_ix = {l:i for i,l in enumerate(target_letters)}\n",
    "print(target_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#특별한 토근들\n",
    "EOS_ix_source=source_letters.index(END)\n",
    "EOS_ix_target=target_letters.index(END)\n",
    "BOS_ix_target = target_letters.index(\" \")\n",
    "PAD_ix=EOS_ix_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작업의 범위를 예측하기위해 단어/번역 길이 분포를 그리자\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAEICAYAAABCsb3rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1fgOygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe6f2ef26d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=[8,4])\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"words\")\n",
    "plt.hist(list(map(len,all_words)),bins=25);\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('translations')\n",
    "plt.hist(list(map(len,all_translations)),bins=25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두번째 단계: 보조 함수들\n",
    "몇가지 헬퍼 함수가 필요하다:\n",
    "- 문자열로부터 데이타를 정수 행렬들로 바꾸기\n",
    "- 샘플 랜덤 미니배치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_matrix(sequences, token_to_i, max_len=None, PAX_ix=PAD_ix):\n",
    "    \"\"\"\n",
    "    가변 길이 토근 시퀀스를 고정된 크기의 행렬로 바꾸자.\n",
    "    사용 예제:\n",
    "    >>>print( as_matrix(words[:3],source_to_ix))\n",
    "    [[15 22 21 28 27 13 -1 -1 -1 -1 -1]\n",
    "     [30 21 15 15 21 14 28 27 13 -1 -1]\n",
    "     [25 37 31 34 21 20 37 21 28 19 13]]\n",
    "    \"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + PAD_ix\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = list(map(token_to_i.get, seq))[:max_len]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_batch(words, word_to_translation, batch_size):\n",
    "    \"\"\"\n",
    "    각 단어에 대해 단어와 랜덤한 올바른 번역의 랜덤 배치를 샘플링하기\n",
    "    예제 사용법:\n",
    "        batch_x, batch_y = sample_batch(train_words, word_to_translations, 10)\n",
    "    \"\"\"\n",
    "    \n",
    "    #단어들 선택하기\n",
    "    batch_words = np.random.choice(words, size=batch_size)\n",
    "    batch_words_len = np.array(list(map(len, batch_words)))\n",
    "    \n",
    "    #번역 선택하기\n",
    "    batch_trans_candidates = list(map(word_to_translation.get, batch_words))\n",
    "    batch_trans = list(map(random.choice, batch_trans_candidates))\n",
    "    batch_trans_len = np.array(list(map(len, batch_trans)))\n",
    "    \n",
    "    return as_matrix(batch_words,source_to_ix), batch_words_len, \\\n",
    "        as_matrix(batch_trans,target_to_ix), batch_trans_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 셋 자르기\n",
    "모든 단어의 20퍼센트는 validation를 위해 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_words,test_words = train_test_split(all_words,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3단계: encoder-decoder를 생성하기\n",
    "아키텍쳐는 두개의 주요 블록으로 구성된다:\n",
    "- Encoder는 단어들을 문자로 읽고 마지막 상태 벡터를 리턴한다.(일반적으로 마지막 RNN 상태 함수)\n",
    "- Decoder는 그 상태 벡터를 가지고 문자단위로 번역을 리턴한다\n",
    "\n",
    "이 섹션에서 encoder를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mask_by_eos(is_eos):\n",
    "    \"\"\"takes indicator of \"it ends now\", returns mask.\n",
    "    Ignores everything after first end.\"\"\"\n",
    "    assert is_eos.ndim==2\n",
    "    is_right_after_eos = np.concatenate([np.zeros_like(is_eos[:,:1]),is_eos[:,:-1]],-1)\n",
    "    is_after_eos = np.eq(np.cumsum(is_right_after_eos,axis=-1),0).astype('uint8')\n",
    "    return is_after_eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "# params\n",
    "HParams = namedtuple(\n",
    "  \"HParams\",\n",
    "  [\n",
    "    \"cell\",\n",
    "    \"batch_size\",\n",
    "    \"layers\",\n",
    "    \"attention\",\n",
    "    \"enc_embedding_dim\",\n",
    "    \"dec_embedding_dim\",\n",
    "    \"hidden_size\",\n",
    "    \"attn_size\",\n",
    "    \"eval_batch_size\",\n",
    "    \"learning_rate\",\n",
    "    \"max_source_len\",\n",
    "    \"max_target_len\",\n",
    "    \"optimizer\",\n",
    "    \"optimizer_clip_gradients\",\n",
    "    \"ckpt_path\"\n",
    "  ])\n",
    "\n",
    "# create params\n",
    "def create_hparams():\n",
    "    return HParams(\n",
    "        cell=tf.contrib.rnn.GRUCell,\n",
    "        batch_size=32,\n",
    "        layers=1,\n",
    "        attention=False,\n",
    "        eval_batch_size=1,\n",
    "        optimizer=\"Adam\",\n",
    "        optimizer_clip_gradients = 10.0,\n",
    "        learning_rate=0.001,\n",
    "        enc_embedding_dim=50,\n",
    "        dec_embedding_dim=50,\n",
    "        hidden_size=512,\n",
    "        attn_size=512,\n",
    "        max_source_len=40,\n",
    "        max_target_len=MAX_OUTPUT_LENGTH,\n",
    "        ckpt_path='./ckpt_dir/model2.ckpt')\n",
    "hparams = create_hparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3', '/gpu:4', '/gpu:5', '/gpu:6', '/gpu:7']\n"
     ]
    }
   ],
   "source": [
    "#check gpu\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "print(get_available_gpus())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here we define functions for model inference (both greedy and sampled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def rl_loss(logits, targets, advantage, weights, name=None):\n",
    "\n",
    "        if len(logits.get_shape()) != 3:\n",
    "            raise ValueError(\"Logits must be a \"\n",
    "                             \"[batch_size x sequence_length x logits] tensor\")\n",
    "        \n",
    "        if len(targets.get_shape()) != 2:\n",
    "            raise ValueError(\"Targets must be a [batch_size x sequence_length] \"\n",
    "                         \"tensor\")\n",
    "        \n",
    "        if len(weights.get_shape()) != 2:\n",
    "            raise ValueError(\"Weights must be a [batch_size x sequence_length] \"\n",
    "                             \"tensor\")\n",
    "        with tf.name_scope(name, \"sequence_loss\", [logits, targets, weights]):\n",
    "            num_classes = tf.shape(logits)[2]\n",
    "            probs_flat = tf.nn.softmax(tf.reshape(logits, [-1, num_classes]))\n",
    "            targets = tf.reshape(targets, [-1])\n",
    "            \n",
    "            probs = tf.reduce_sum(probs_flat * tf.one_hot(targets, depth=num_classes), 1)\n",
    "            logprobs = tf.log(probs)\n",
    "            \n",
    "            crossent = -logprobs * tf.reshape(weights, [-1])\n",
    "            \n",
    "            batch_size = tf.shape(logits)[0]\n",
    "            sequence_length = tf.shape(logits)[1]\n",
    "            crossent = tf.reshape(crossent, [batch_size, sequence_length])\n",
    "\n",
    "            J = crossent * advantage[:, None] \n",
    "\n",
    "            loss = tf.reduce_sum(J)\n",
    "            total_size = tf.reduce_sum(weights)\n",
    "            loss /= total_size\n",
    "\n",
    "            #regularize with negative entropy\n",
    "            entropy = -probs * logprobs \n",
    "            entropy = tf.reduce_sum(entropy)\n",
    "            total_size = tf.reduce_sum(weights)\n",
    "            entropy /= total_size\n",
    "\n",
    "            loss -=  0.01*entropy\n",
    "        \n",
    "            return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "from sample_embedding_helper import SampleEmbeddingHelper\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "\n",
    "import os\n",
    "\n",
    "class Seq2SeqModel:\n",
    "    def __init__(self, config, mode='training'):\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        self.max_target_len = self.config.max_target_len\n",
    "        self.available_gpus = get_available_gpus()\n",
    "        self.current_gpu_index = 0\n",
    "        self.total_gpu_cnt = len(self.available_gpus)\n",
    "        \n",
    "    def _get_next_gpu(self):\n",
    "        if self.total_gpu_cnt == 0:\n",
    "            return 'cpu:0'\n",
    "        else:\n",
    "            self.current_gpu_index += 1\n",
    "            self.current_gpu_index %= (self.total_gpu_cnt + 1)\n",
    "            return self.available_gpus[self.current_gpu_index-1]\n",
    "        \n",
    "    # add placeholders\n",
    "    def _add_placeholders(self):\n",
    "        self.input_ids = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=[None, None],\n",
    "            name='input_ids')\n",
    "\n",
    "        self.inputs_len = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=[None],\n",
    "            name='inputs_len')\n",
    "        \n",
    "        if self.mode == 'training':\n",
    "            self.target_ids = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[None, None],\n",
    "                name='target_ids')\n",
    "\n",
    "            self.targets_len = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[None],\n",
    "                name='targets_len')\n",
    "                      \n",
    "            self.rl_enable = tf.placeholder(\n",
    "                tf.bool,\n",
    "                shape=None,\n",
    "                name='rl_enable'\n",
    "            )\n",
    "            \n",
    "            self.targets_length = tf.minimum(self.targets_len, self.config.max_target_len)\n",
    "            #self.max_target_len = tf.reduce_max(self.targets_length, name='max_target_len')\n",
    "            self.targets = self.target_ids[:, :self.max_target_len]\n",
    "        #else:\n",
    "                    \n",
    "    # build a computation graph\n",
    "    def build_graph(self, saver=None):\n",
    "        # add placeholder variables\n",
    "        self._add_placeholders()\n",
    "\n",
    "        # build encoder\n",
    "        self._add_encoder()\n",
    "        \n",
    "        # build decoder\n",
    "        self._add_decoder()\n",
    "            \n",
    "        #optimizer\n",
    "        if self.mode == 'training':\n",
    "            self._add_optimizer()\n",
    "        \n",
    "        if saver:\n",
    "            self.saver = saver\n",
    "        else:\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "    # encoder layer\n",
    "    def _add_encoder(self):\n",
    "        with tf.variable_scope('Encoder') as scope:\n",
    "            self.batch_size = tf.shape(self.input_ids)[0]\n",
    "\n",
    "            enc_W_emb = tf.get_variable('en_embedding', \n",
    "                                        initializer=tf.random_uniform([len(source_letters), self.config.enc_embedding_dim]),\n",
    "                                        dtype=tf.float32)\n",
    "            \n",
    "            enc_emb_inputs = tf.nn.embedding_lookup(\n",
    "                enc_W_emb, self.input_ids, name='emb_inputs')\n",
    "\n",
    "            # bidirectional rnn\n",
    "            if self.config.layers == 1:\n",
    "                enc_cell = tf.contrib.rnn.DeviceWrapper(\n",
    "                    self.config.cell(self.config.hidden_size), \n",
    "                    device=self._get_next_gpu())\n",
    "                \n",
    "                self.enc_outputs, self.enc_last_state = tf.nn.dynamic_rnn(\n",
    "                    cell=enc_cell,\n",
    "                    inputs=enc_emb_inputs,\n",
    "                    sequence_length=self.inputs_len,\n",
    "                    time_major=False,\n",
    "                    dtype=tf.float32)                \n",
    "            else:\n",
    "                enc_cell_fw = self.config.cell(self.config.hidden_size)\n",
    "                enc_cell_fw = tf.contrib.rnn.DeviceWrapper(enc_cell_fw, device=self._get_next_gpu())\n",
    "\n",
    "                gpu2 = self._get_next_gpu()\n",
    "\n",
    "                enc_cell_bw = self.config.cell(self.config.hidden_size)\n",
    "                enc_cell_bw = tf.contrib.rnn.DeviceWrapper(enc_cell_bw, device=gpu2)\n",
    "\n",
    "                enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(enc_cell_fw, \n",
    "                                                                           enc_cell_bw, \n",
    "                                                                           enc_emb_inputs,\n",
    "                                                                           self.inputs_len,\n",
    "                                                                           dtype=tf.float32)\n",
    "                enc_outputs = tf.concat(enc_outputs,2)\n",
    "\n",
    "                enc_cell = tf.contrib.rnn.DeviceWrapper(\n",
    "                        self.config.cell(num_units=self.config.hidden_size),\n",
    "                        device=gpu2)\n",
    "\n",
    "                # multi rnn\n",
    "                if self.config.layers > 2:\n",
    "                    enc_cell = [enc_cell]\n",
    "                    for _ in range(self.config.layers-2):\n",
    "                        enc_cell.append(tf.contrib.rnn.DeviceWrapper(\n",
    "                            tf.contrib.rnn.ResidualWrapper(\n",
    "                                self.config.cell(num_units=self.config.hidden_size)),\n",
    "                            device=self._get_next_gpu()))\n",
    "\n",
    "                    enc_cell = tf.contrib.rnn.MultiRNNCell(enc_cell)\n",
    "                    \n",
    "                \n",
    "                self.enc_outputs, self.enc_last_state = tf.nn.dynamic_rnn(\n",
    "                    cell=enc_cell,\n",
    "                    inputs=enc_outputs,\n",
    "                    sequence_length=self.inputs_len,\n",
    "                    time_major=False,\n",
    "                    dtype=tf.float32)\n",
    "                \n",
    "                self.enc_last_state = (enc_state[0],) + self.enc_last_state\n",
    "                \n",
    "    # decoder layer\n",
    "    def _add_decoder(self):\n",
    "        with tf.variable_scope('Decoder') as scope:\n",
    "            cells = []\n",
    "            if self.config.layers > 1:\n",
    "                for i in range(self.config.layers):\n",
    "                    if i == 0:\n",
    "                        cells.append(tf.contrib.rnn.DeviceWrapper(\n",
    "                            self.config.cell(self.config.hidden_size), \n",
    "                            device=self._get_next_gpu()))\n",
    "                    else:\n",
    "                        cells.append(tf.contrib.rnn.DeviceWrapper(\n",
    "                            tf.contrib.rnn.ResidualWrapper(\n",
    "                                self.config.cell(num_units=self.config.hidden_size)),\n",
    "                            device=self._get_next_gpu()))\n",
    "\n",
    "                self.dec_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "            else:\n",
    "                self.dec_cell = tf.contrib.rnn.DeviceWrapper(\n",
    "                                    self.config.cell(self.config.hidden_size), \n",
    "                                    device=self._get_next_gpu())\n",
    "            \n",
    "            if self.config.attention:\n",
    "                attn_mech = tf.contrib.seq2seq.LuongAttention(\n",
    "                        num_units=self.config.attn_size,\n",
    "                        memory=self.enc_outputs,\n",
    "                        memory_sequence_length=self.inputs_len,\n",
    "                        normalize=False,\n",
    "                        name='LuongAttention')\n",
    "\n",
    "                self.dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(\n",
    "                        cell=self.dec_cell,\n",
    "                        attention_mechanism=attn_mech,\n",
    "                        attention_size=self.config.attn_size,\n",
    "                        # attention_history=False (in ver 1.2)\n",
    "                        name='Attention_Wrapper')\n",
    "\n",
    "                # last_enc_state wrapper\n",
    "                self.initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(\n",
    "                        cell_state=self.enc_last_state,\n",
    "                        attention=_zero_state_tensors(self.config.attn_size, self.batch_size, tf.float32))\n",
    "            else:\n",
    "                self.initial_state = self.enc_last_state\n",
    "\n",
    "            self.dec_W_emb = tf.get_variable('de_embedding', \n",
    "                                             initializer=tf.random_uniform([len(target_letters),\n",
    "                                                                            self.config.dec_embedding_dim]),\n",
    "                                             dtype=tf.float32)\n",
    "                \n",
    "            self.output_layer = Dense(len(target_letters), name='output_projection')\n",
    "            if self.mode == 'training': # training layer\n",
    "                self.sampling_predictions, self.greedy_predictions, self.rl_loss = self._add_rl_training_layer(scope)\n",
    "                self.predictions, self.loss = self._add_training_layer(scope)\n",
    "                \n",
    "            else: # inference layer\n",
    "                self.predictions = self._add_inference_layer()     \n",
    "\n",
    "    # inference layer\n",
    "    def _add_inference_layer(self):\n",
    "        # inference layer\n",
    "        sequence_start = [BOS_ix_target]\n",
    "        start_tokens = tf.tile(sequence_start, [self.batch_size], name='start_tokens')\n",
    "\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding=self.dec_W_emb,\n",
    "            start_tokens=start_tokens,\n",
    "            end_token=EOS_ix_target) \n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell=self.dec_cell,\n",
    "            helper=inference_helper,\n",
    "            initial_state=self.initial_state,\n",
    "            output_layer=self.output_layer)\n",
    "\n",
    "        inference_outputs, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            inference_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=self.max_target_len)\n",
    "            \n",
    "        predictions = inference_outputs.sample_id\n",
    "        return predictions\n",
    "    \n",
    "    # training layer\n",
    "    def _add_training_layer(self, scope):\n",
    "        # training_layer\n",
    "        self.max_target_len = tf.reduce_max(self.targets_length, name='max_target_len')\n",
    "        \n",
    "        dec_inputs = tf.concat([tf.zeros_like(self.targets[:,:1])+BOS_ix_target,\n",
    "                                          self.targets[:,:-1]],axis=1)\n",
    "        \n",
    "        dec_emb_inputs = tf.nn.embedding_lookup(\n",
    "            self.dec_W_emb, dec_inputs, name='emb_inputs')\n",
    "\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            inputs=dec_emb_inputs,\n",
    "            sequence_length=self.targets_length,\n",
    "            time_major=False,\n",
    "            name='training_helper')\n",
    "\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell=self.dec_cell,\n",
    "            helper=training_helper,\n",
    "            initial_state=self.initial_state,\n",
    "            output_layer=self.output_layer)\n",
    "\n",
    "        max_target_len = tf.reduce_max(self.targets_length, name='max_target_len')\n",
    "        \n",
    "        train_outputs, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            training_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=max_target_len)\n",
    "            \n",
    "        # predictions\n",
    "        predictions = train_outputs.sample_id\n",
    "\n",
    "        masks = tf.sequence_mask(self.targets_length, self.max_target_len, dtype=tf.float32, name='masks')\n",
    "\n",
    "        # loss\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits=train_outputs.rnn_output, \n",
    "                                                     targets=self.targets,\n",
    "                                                     weights=masks, name='batch_loss')        \n",
    "        return predictions, loss\n",
    "        \n",
    "    def _add_rl_training_layer(self, scope):\n",
    "        # RL training_layer\n",
    "        sequence_start = [BOS_ix_target]\n",
    "        start_tokens = tf.tile(sequence_start, [self.batch_size], name='start_tokens')\n",
    "\n",
    "        #greedy decoding\n",
    "        greedy_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding=self.dec_W_emb,\n",
    "            start_tokens=start_tokens,\n",
    "            end_token=EOS_ix_target) \n",
    "\n",
    "        greedy_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell=self.dec_cell,\n",
    "            helper=greedy_helper,\n",
    "            initial_state=self.initial_state,\n",
    "            output_layer=self.output_layer)\n",
    "\n",
    "        greedy_dec_outputs, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            greedy_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=self.max_target_len)\n",
    "\n",
    "        # greedy predictions\n",
    "        greedy_predictions = greedy_dec_outputs.sample_id\n",
    "        \n",
    "        scope.reuse_variables()\n",
    "\n",
    "        #sampling decoding\n",
    "        training_sampling_helper = SampleEmbeddingHelper(\n",
    "            embedding=self.dec_W_emb,\n",
    "            start_tokens=start_tokens,\n",
    "            end_token=EOS_ix_target)\n",
    "\n",
    "        training_sampling_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell=self.dec_cell,\n",
    "            helper=training_sampling_helper,\n",
    "            initial_state=self.initial_state,\n",
    "            output_layer=self.output_layer)\n",
    "\n",
    "        train_outputs, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            training_sampling_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=self.max_target_len)\n",
    "\n",
    "        #seq_len = tf.shape(train_outputs.rnn_output)[1]\n",
    "\n",
    "        # predictions\n",
    "        predictions = train_outputs.sample_id\n",
    "        \n",
    "        # mask\n",
    "        masks = tf.sign(tf.to_float(predictions))\n",
    "\n",
    "        # advantage\n",
    "        self.advantage = tf.py_func(self._calculate_advantage,\n",
    "                               [self.input_ids,\n",
    "                                predictions,\n",
    "                                greedy_predictions], Tout=tf.float32)\n",
    "        self.advantage = tf.reshape(self.advantage, [self.batch_size])\n",
    "        # loss\n",
    "        loss = rl_loss(logits=train_outputs.rnn_output, \n",
    "                       targets=predictions,\n",
    "                       advantage=self.advantage, \n",
    "                       weights=masks, \n",
    "                       name='rl_loss')\n",
    "        \n",
    "        return predictions, greedy_predictions, loss\n",
    "\n",
    "    # optimizing layer\n",
    "    def _add_optimizer(self):\n",
    "        with tf.variable_scope('Optimizer') as scope:\n",
    "            def _clip_gradients(grads_and_vars):\n",
    "                \"\"\"Clips gradients by global norm.\"\"\"\n",
    "                gradients, variables = zip(*grads_and_vars)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "                    gradients, self.config.optimizer_clip_gradients)\n",
    "                return list(zip(clipped_gradients, variables))\n",
    "            loss = tf.cond(self.rl_enable, lambda : self.rl_loss, lambda : self.loss)\n",
    "            \n",
    "            self.train_op = tf.contrib.layers.optimize_loss(loss=loss, \n",
    "                                                            global_step=tf.contrib.framework.get_global_step(),\n",
    "                                                            learning_rate=self.config.learning_rate, \n",
    "                                                            clip_gradients=_clip_gradients,\n",
    "                                                            optimizer=self.config.optimizer)\n",
    "\n",
    "    # translate a word\n",
    "    def translate(self, sess, word, sample=False):\n",
    "        #if os.path.isfile(self.config.ckpt_path):\n",
    "        #    self.restore(sess)\n",
    "            \n",
    "        assert word.endswith(END)\n",
    "        word_len = np.array([len(word)])\n",
    "        word_ix = as_matrix([word.lower()],source_to_ix)\n",
    "        feed_dict = {self.input_ids: word_ix, self.inputs_len: word_len}\n",
    "        if sample:\n",
    "            outputs = sess.run(self.predictions, feed_dict=feed_dict)\n",
    "        else:\n",
    "            outputs = sess.run(self.predictions, feed_dict=feed_dict)\n",
    "        trans = list(map(target_letters.__getitem__, outputs[0]))\n",
    "        if END in trans:\n",
    "            trans = trans[:trans.index(END)+1]\n",
    "            \n",
    "        return ''.join(trans)\n",
    "    \n",
    "    # infer method(for a batch)\n",
    "    def infer(self, sess, inputs, inputs_len):\n",
    "        #if os.path.isfile(self.config.ckpt_path):\n",
    "        #    self.restore(sess)\n",
    "        \n",
    "        feed_dict = {self.input_ids: inputs, self.inputs_len: inputs_len}\n",
    "        \n",
    "        outputs = sess.run(self.predictions, feed_dict=feed_dict)\n",
    "    \n",
    "        return outputs\n",
    "    \n",
    "    # train a step\n",
    "    def train_step(self, sess, inputs, inputs_len, targets, targets_len):\n",
    "        feed_dict = {self.input_ids: inputs, self.inputs_len: inputs_len,\n",
    "                    self.target_ids: targets, self.targets_len: targets_len,\n",
    "                    self.rl_enable: False}\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "        #loss = sess.run(self.advantage, feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def summary(self):\n",
    "        summary_writer = tf.summary.FileWriter(\n",
    "            logdir=self.config.ckpt_path, graph=tf.get_default_graph())\n",
    "               \n",
    "    def restore(self, sess, ckpt_path=None):\n",
    "        if ckpt_path:\n",
    "            self.saver.restore(sess, ckpt_path)\n",
    "        else:\n",
    "            self.saver.restore(sess, self.config.ckpt_path)\n",
    "        print('Restore Finished!')\n",
    "        \n",
    "    def save(self, sess, save_path=None):\n",
    "        if save_path:\n",
    "            self.saver.save(sess, save_path)\n",
    "            print(f'Saving model at {save_path}')\n",
    "        else:\n",
    "            self.saver.save(sess, self.config.ckpt_path)\n",
    "            print(f'Saving model at {save_path}')\n",
    "\n",
    "            \n",
    "    def rl_train_step(self, sess, inputs, inputs_len, targets, targets_len):\n",
    "        \n",
    "        feed_dict = {self.input_ids: inputs, self.inputs_len: inputs_len,\n",
    "                    self.target_ids: targets, self.targets_len: targets_len,\n",
    "                    self.rl_enable: True}\n",
    "        \n",
    "        #sampling_preds, greedy_preds = sess.run([self.predictions, self.greedy_predictions], \n",
    "        #                                        feed_dict=feed_dict)\n",
    "        \n",
    "        #print(sampling_preds)\n",
    "        #print(greedy_preds)\n",
    "        #sampling_masks = get_mask_by_eos(np.equal(sampling_preds,EOS_ix_target))\n",
    "        #greedy_masks = get_mask_by_eos(np.equal(greedy_preds,EOS_ix_target))\n",
    "        #input_masks = get_mask_by_eos(np.equal(inputs, EOS_ix_source))\n",
    "        \n",
    "        #rewards = -compute_levenshtein(inputs, input_masks, sampling_preds, sampling_masks)\n",
    "        #baseline = -compute_levenshtein(inputs, input_masks, greedy_preds, greedy_masks)\n",
    "\n",
    "        #advantage = rewards - baseline\n",
    "        #print(advantage)\n",
    "        \n",
    "        #feed_dict.update({self.advantage:advantage})\n",
    "\n",
    "        preds, _, loss = sess.run([self.predictions, self.train_op, self.loss], feed_dict=feed_dict)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def _calculate_advantage(self, inputs, sampling_preds, greedy_preds):\n",
    "        \n",
    "        sampling_masks = get_mask_by_eos(np.equal(sampling_preds,EOS_ix_target))\n",
    "        greedy_masks = get_mask_by_eos(np.equal(greedy_preds,EOS_ix_target))\n",
    "        input_masks = get_mask_by_eos(np.equal(inputs, EOS_ix_source))\n",
    "        \n",
    "        rewards = -compute_levenshtein(inputs, input_masks, sampling_preds, sampling_masks)\n",
    "        baseline = -compute_levenshtein(inputs, input_masks, greedy_preds, greedy_masks)\n",
    "\n",
    "        advantage = rewards - baseline\n",
    "        return advantage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:אנרכיזם;\n",
      "y_sampled:ıلللqqqqṇṇṇááףqqqqqq\n",
      "y_greedy:ıلللqqqqṇṇṇááףqqqqqq\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test untrained model\n",
    "#should be random\n",
    "tf.reset_default_graph()\n",
    "print ('x:'+all_words[0])\n",
    "seq2seq = Seq2SeqModel(hparams, mode='inference')\n",
    "seq2seq.build_graph()\n",
    "with tf.Session() as sess:\n",
    "    #seq2seq.restore(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print ('y_sampled:'+seq2seq.translate(sess, all_words[0],sample=True))\n",
    "    print ('y_greedy:'+seq2seq.translate(sess, all_words[0]))\n",
    "#praise Cthulhu!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score 함수\n",
    "LogLikelihood는 모델 성능 평가에 좋지 않다.\n",
    "- 제로 확률이 한번 예측 되면, 전체 모델을 망치지 말아야 한다.\n",
    "- 여러개의 바른 답이 있다면 단지 하나의 번역을 배우는 것만으로 충분하다.\n",
    "- 한 단계마다 가장 있을법한 음소를 가지는지 출력해???\n",
    "\n",
    "그러므로, 우리는 최소 Levenshten distance를 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "def get_distance(word, trans):\n",
    "    \"\"\"\n",
    "    워드와 예측된 번역을 입력으로 하고 바른 번역에 얼마나 가까운지 edit distance를 평가한다.\n",
    "    \"\"\"\n",
    "    #print(word)\n",
    "    references = word_to_translation[word]\n",
    "    #print(references)\n",
    "    #print(trans)\n",
    "    assert len(references) !=0, \"word/unknown word\"\n",
    "    return min(editdistance.eval(trans, ref) for ref in references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(sess, model, bsize=100):\n",
    "    \"\"\"\n",
    "    bszie의 랜덤 샘플에 대해 levenshtein distance를 계산하는 함수\n",
    "    \"\"\"\n",
    "    batch_words = np.random.choice(test_words, size=(bsize,)) \n",
    "    #for word in batch_words:\n",
    "    #    print(word)\n",
    "    predictions = [model.translate(sess, word) for word in batch_words]\n",
    "    #for p in predictions:\n",
    "    #    print(p)\n",
    "    #print(predictions)\n",
    "    distances = [get_distance(word, prediction) for (word, prediction) in zip(batch_words, predictions)]    \n",
    "    return np.array(distances,dtype='float32')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.959999, 19.959999, 19.959999, 19.92, 19.9, 19.940001, 19.93, 19.92, 19.92, 19.959999]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #should be around 5-50 and decrease rapidly :)\n",
    "    result = [score(sess, seq2seq, 100).mean() for _ in range(10)]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Supervised pre-training\n",
    "여기에서 로그 우도를 최대화 함으로써 모델을 학습하는 함수를 정의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAEICAYAAAByPazKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJW0REZE0oaQtIiKSJpS0RURE0sT/A5qEJnR4YPO1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe338945f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llh=1.419, mean score=6.768\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_dir/model2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 2900/25000 [08:40<8:53:45,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2934/25000 [08:45<47:24,  7.76it/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-e9d9f7c6dc08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     loss_history.append(\n\u001b[0;32m---> 15\u001b[0;31m              seq2seq.train_step(sess, *sample_batch(train_words,word_to_translation,32)))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mREPORT_FREQ\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-f36b65869406>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, sess, inputs, inputs_len, targets, targets_len)\u001b[0m\n\u001b[1;32m    387\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                     self.rl_enable: False}\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;31m#loss = sess.run(self.advantage, feed_dict=feed_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm,trange #or use tqdm_notebook,tnrange\n",
    "\n",
    "loss_history=[]\n",
    "editdist_history = []\n",
    "tf.reset_default_graph()\n",
    "seq2seq = Seq2SeqModel(hparams)\n",
    "seq2seq.build_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#seq2seq.restore(saver, sess)\n",
    "for i in trange(25000):\n",
    "    loss_history.append(\n",
    "             seq2seq.train_step(sess, *sample_batch(train_words,word_to_translation,32)))\n",
    "        \n",
    "    if (i+1)%REPORT_FREQ==0:\n",
    "    #if False:\n",
    "            seq2seq.save(sess)\n",
    "            sess.close()\n",
    "            \n",
    "            tf.reset_default_graph()\n",
    "            sess = tf.Session()\n",
    "            \n",
    "            seq2seq_inference = Seq2SeqModel(hparams, mode='inference')\n",
    "            seq2seq_inference.build_graph()\n",
    "            seq2seq_inference.restore(sess)\n",
    "                \n",
    "            clear_output(True)\n",
    "            current_scores = score(sess, seq2seq_inference)\n",
    "            editdist_history.append(current_scores.mean())\n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.subplot(121)\n",
    "            plt.title('val score distribution')\n",
    "            plt.hist(current_scores, bins = 20)\n",
    "            plt.subplot(122)\n",
    "            plt.title('val score / traning time')\n",
    "            plt.plot(editdist_history)\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "            print(\"llh=%.3f, mean score=%.3f\"%(np.mean(loss_history[-10:]),np.mean(editdist_history[-10:])))\n",
    "            tf.reset_default_graph()\n",
    "            seq2seq = Seq2SeqModel(hparams)\n",
    "            seq2seq.build_graph()\n",
    "            sess = tf.Session()\n",
    "            seq2seq.restore(sess)\n",
    "\n",
    "seq2seq.save(sess)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt_dir/model2.ckpt\n",
      "Restore Finished!\n",
      "עלי נאסר; -> ali nesser;\n",
      "קטגוריה:בלפסט; -> belfstet;\n",
      "ארבינקא; -> arbinaka;\n",
      "זיכרון נוהלי; -> zicron nolly;\n",
      "scar tissue; -> scart stisso;\n",
      "עמק האלבה בדרזדן; -> amar bender;\n",
      "ז'ורז' בולנז'ה; -> georges bollanze;\n",
      "מילקן; -> milken;\n",
      "כביש 73; -> housh 30;\n",
      "אלין; -> elin;\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "seq2seq_inference = Seq2SeqModel(hparams, mode='inference')\n",
    "seq2seq_inference.build_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    seq2seq_inference.restore(sess)\n",
    "    \n",
    "    for word in train_words[:10]:\n",
    "        print(\"%s -> %s\"%(word,seq2seq_inference.translate(sess, word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5: 정책 그라디언트(Policy gradient )\n",
    "\n",
    "손실(loss) 함수를 정의 할 필요가 있다.\n",
    "\n",
    "우리 작업은 단어와 번역 행렬들을 입력으로 가지고 그것들을 실제 단어 그리고 음절로 변환하고 위에 get_distance 함수를 통해서 min-levenshtein를 계산하는 _compute_levenshtein을 구현하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mask_by_eos(is_eos):\n",
    "    \"\"\" \"지금 부터 끝이다\"의 표시를 가지고, mask를 리턴한다. \n",
    "    첫번째 마지막이후로 모든것을 무시한다. \n",
    "    [[False,False,True,True]] -> [[1,1,1,0]]\n",
    "    \"\"\"\n",
    "    assert is_eos.ndim==2\n",
    "    is_right_after_eos = np.concatenate([np.zeros_like(is_eos[:,:1]),is_eos[:,:-1]],-1)\n",
    "    is_after_eos = np.equal(np.cumsum(is_right_after_eos,axis=-1), 0).astype('uint8')\n",
    "    return is_after_eos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_levenshtein(words_ix,words_mask,trans_ix,trans_mask):\n",
    "    \"\"\"\n",
    "    예측된 번역에 대한 levenshtein 손실을 계산하는 커스텀 theano 연산이다.\n",
    "    \n",
    "    Params:\n",
    "    - words_ix - 문자 인덱스 행렬, shape=[batch_size, word_length]\n",
    "    - words_mask - 0과1로 구성된 행렬,\n",
    "        1은 단어는 아직 끝나지 않았음을 의미한다.\n",
    "        0은 단어는 이미 끝나고 이것은 패딩이다는 것을 의미한다.\n",
    "    -trans_ix - 결과 문자 인덱스 행렬, shape=[batch_size,translation_length]\n",
    "    -trans_mask - 0과1로 구성된 행렬 word_mask와 유사하다. 그러나 tans_ix를 위한 것이다.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    words = []\n",
    "    for i, letters_ix in enumerate(words_ix):\n",
    "        word = ''.join([source_letters[letter_ix] for letter_ix in letters_ix][:sum(words_mask[i])])\n",
    "        words.append(word)\n",
    "        \n",
    "    assert type(words) is list\n",
    "    assert type(words[0]) is str \n",
    "    assert len(words)==len(words_ix)\n",
    "    \n",
    "    #convert translations to lists    \n",
    "    translations = []\n",
    "    for i, tran_ix in enumerate(trans_ix):\n",
    "        tran = ''.join([target_letters[letter_ix] for letter_ix in tran_ix][:sum(trans_mask[i])])\n",
    "        translations.append(tran)\n",
    "    \n",
    "        \n",
    "    assert type(translations) is list\n",
    "    assert type(translations[0]) is str\n",
    "    assert len(translations)==len(trans_ix)\n",
    "\n",
    "    distances = [get_distance(w,t) for w, t in zip(words, translations)]\n",
    "    \n",
    "    assert type(distances) in (list,tuple,np.ndarray) and len(distances) == len(words_ix)\n",
    "    \n",
    "    distances = np.array(list(distances),dtype='float32')\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test suite\n",
    "#sample random batch of (words, correct trans, wrong trans)\n",
    "batch_words = np.random.choice(train_words, size=100 )\n",
    "batch_trans = list(map(random.choice,map(word_to_translation.get,batch_words )))\n",
    "batch_trans_wrong = np.random.choice(all_translations,size=100)\n",
    "\n",
    "batch_words_ix = as_matrix(batch_words,source_to_ix)\n",
    "batch_trans_ix = as_matrix(batch_trans,target_to_ix)\n",
    "batch_trans_wrong_ix = as_matrix(batch_trans_wrong,target_to_ix)\n",
    "\n",
    "batch_words_mask = get_mask_by_eos(np.equal(batch_words_ix,EOS_ix_source))\n",
    "batch_trans_mask = get_mask_by_eos(np.equal(batch_trans_ix,EOS_ix_target))\n",
    "batch_trans_wrong_mask = get_mask_by_eos(np.equal(batch_trans_wrong_ix,EOS_ix_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems alright!\n"
     ]
    }
   ],
   "source": [
    "#assert compute_levenshtein is zero for ideal translations\n",
    "correct_answers_score = compute_levenshtein(batch_words_ix,batch_words_mask,\n",
    "                                            batch_trans_ix,batch_trans_mask)\n",
    "\n",
    "assert np.all(correct_answers_score==0),\"a perfect translation got nonzero levenshtein score!\"\n",
    "\n",
    "print(\"Everything seems alright!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems alright!\n"
     ]
    }
   ],
   "source": [
    "#assert compute_levenshtein matches actual scoring function\n",
    "wrong_answers_score = compute_levenshtein(batch_words_ix,batch_words_mask,\n",
    "                                            batch_trans_wrong_ix,batch_trans_wrong_mask)\n",
    "\n",
    "true_wrong_answers_score = np.array(list(map(get_distance,batch_words,batch_trans_wrong)))\n",
    "\n",
    "assert np.all(wrong_answers_score==true_wrong_answers_score),\"for some word symbolic levenshtein is different from actual levenshtein distance\"\n",
    "\n",
    "print(\"Everything seems alright!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-critical policy gradient\n",
    "이 섹션에서, self-critical sequence학습이라고 하는 알고리즘을 구현할 것이다.\n",
    "이 알고리즘은 특별한 베이스라인을 가진 바닐라 정책 그라디언트이다.\n",
    "\n",
    "$$ \\nabla J = E_{x \\sim p(s)} E_{y \\sim \\pi(y|x)} \\nabla log \\pi(y|x) \\cdot (R(x,y) - b(x)) $$\n",
    "\n",
    "여기서 R(x,y)는 네가티브 levenshtein distance 이다(그것을 최소화 하기 때문이다). 베이스라인 b(x)는 단어 x에 대하 모델이 얼마나 되는지 나타낸다. 실제적으로, 이것은 그라디(greedy) 번역 점수로써 베이스라인을 계산한다는 것을 의미한다, $b(x) = R(x,y_{greedy}(x)) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create params\n",
    "def create_hparams():\n",
    "    return HParams(\n",
    "        cell=hparams.cell,\n",
    "        batch_size=hparams.batch_size,\n",
    "        layers=hparams.layers,\n",
    "        attention=hparams.attention,\n",
    "        eval_batch_size=hparams.eval_batch_size,\n",
    "        optimizer=hparams.optimizer,\n",
    "        optimizer_clip_gradients = hparams.optimizer_clip_gradients,\n",
    "        learning_rate=0.00001,\n",
    "        enc_embedding_dim=hparams.enc_embedding_dim,\n",
    "        dec_embedding_dim=hparams.dec_embedding_dim,\n",
    "        hidden_size=hparams.hidden_size,\n",
    "        attn_size=hparams.attn_size,\n",
    "        max_source_len=hparams.max_source_len,\n",
    "        max_target_len=MAX_OUTPUT_LENGTH,\n",
    "        ckpt_path='./ckpt_dir/model2.ckpt')\n",
    "\n",
    "hparams2 = create_hparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책 그라디언트 학습(Policy gradient training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAEICAYAAACOKIcAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZBERkQzw/wOOOgcLbhn5PAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2fa78e5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J=6.927, mean score=6.564\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_dir/model2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 1800/100000 [04:53<36:49:28,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1830/100000 [04:56<3:02:43,  8.95it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-07177263f5ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_translation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         loss_history.append(\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             )\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(\"infer:\", seq2seq_inferencer.infer(sess, inputs, inputs_len))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-f36b65869406>\u001b[0m in \u001b[0;36mrl_train_step\u001b[0;34m(self, sess, inputs, inputs_len, targets, targets_len)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m#feed_dict.update({self.advantage:advantage})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hanmail/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#REPORT_FREQ=1\n",
    "tf.reset_default_graph()\n",
    "seq2seq = Seq2SeqModel(hparams2)\n",
    "seq2seq.build_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "seq2seq.restore(sess)\n",
    "    #tvars = tf.trainable_variables()\n",
    "    #tvars_vals = sess.run(tvars)\n",
    "    \n",
    "    #for var, val in zip(tvars, tvars_vals):\n",
    "    #    print(var.name, val)\n",
    "    \n",
    "for i in trange(100000):\n",
    "        #for i in trange(200):\n",
    "        inputs, inputs_len, targets, targets_len = sample_batch(train_words, word_to_translation,32)\n",
    "        loss_history.append(\n",
    "            seq2seq.rl_train_step(sess, inputs, inputs_len, targets, targets_len)\n",
    "            )\n",
    "        #print(\"infer:\", seq2seq_inferencer.infer(sess, inputs, inputs_len))\n",
    "        \n",
    "        if (i+1)%REPORT_FREQ==0:\n",
    "        #if (i+1)%10 == 0:\n",
    "            #seq2seq.save(sess)\n",
    "            sess.close()\n",
    "            \n",
    "            tf.reset_default_graph()\n",
    "            seq2seq_inferencer = Seq2SeqModel(hparams2, mode='inference')\n",
    "            seq2seq_inferencer.build_graph()\n",
    "            sess = tf.Session()\n",
    "            seq2seq_inferencer.restore(sess)\n",
    "            \n",
    "            clear_output(True)\n",
    "            current_scores = score(sess, seq2seq_inferencer)\n",
    "            editdist_history.append(current_scores.mean())\n",
    "            plt.figure(figsize=(8,4))\n",
    "            plt.subplot(121)\n",
    "            plt.title('val score distribution')\n",
    "            plt.hist(current_scores, bins = 20)\n",
    "            plt.subplot(122)\n",
    "            plt.title('val score / traning time')\n",
    "            plt.plot(editdist_history)\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "            print(\"J=%.3f, mean score=%.3f\"%(np.mean(loss_history[-10:]),np.mean(editdist_history[-10:])))\n",
    "            sess.close()\n",
    "            tf.reset_default_graph()\n",
    "            \n",
    "            seq2seq = Seq2SeqModel(hparams2)\n",
    "            seq2seq.build_graph()\n",
    "\n",
    "            sess = tf.Session()\n",
    "            seq2seq.restore(sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt_dir/model.ckpt\n",
      "Restore Finished!\n",
      "example;\n"
     ]
    }
   ],
   "source": [
    "seq2seq_inferencer = Seq2SeqModel(hparams2, mode='inference')\n",
    "seq2seq_inferencer.build_graph()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    seq2seq_inferencer.restore(saver, sess)\n",
    "    print(seq2seq_inferencer.translate(sess, \"EXAMPLE;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicted_translations = list(map(model.translate,tqdm(test_words)))\n",
    "#distances = map(get_score,test_words,predicted_translations)\n",
    "\n",
    "#print (\"Mean Levenshtein distance:\",np.mean(distances))\n",
    "#print (\"Median Levenshtein distance:\",np.median(distances))\n",
    "#plt.hist(distances,range=[0,10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
